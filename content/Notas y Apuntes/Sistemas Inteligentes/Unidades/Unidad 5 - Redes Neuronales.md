---
title: Unidad 5 - Redes Neuronales
draft: true
tags:
---

----
# [[Contenido del curso|ğŸ  Inicio - Sistemas Inteligentes]]

----
# Redes neuronales

>[!example] Ejercicio
>Colocar 5 caracterÃ­sticas que tu consideres para comprar un auto y ponerle una ponderaciÃ³n de importancia. La suma de esos factores debe ser 1
## Criterios para comprar un auto ğŸš—

| CaracterÃ­stica           | PonderaciÃ³n |
| ------------------------ | ----------- |
| Precio (barato)          | 0.4         |
| Facilidad de refacciones | 0.3         |
| Modelo                   | 0.1         |
| Eficiencia               | 0.15        |
| Marca                    | 0.05        |
Elegir una calificaciÃ³n mÃ­nima que personalmente consideres para adquirirlo.

## CalificaciÃ³n minima â¡ï¸ 8

Comparar entre estos dos autos siguiendo la ponderaciÃ³n que estableciste.

| x                        | PonderaciÃ³n | Versa | Mustang |
| ------------------------ | ----------- | ----- | ------- |
| Precio (barato)          | 0.4         | 1     | 0       |
| Facilidad de refacciones | 0.3         | 1     | 0       |
| Modelo                   | 0.1         | 0     | 1       |
| Eficiencia               | 0.15        | 1     | 0       |
| Marca                    | 0.05        | 1     | 1       |
| **Total**                |             | 9     | 1.5     |

----
# Redes neuronales
## DefiniciÃ³n

>[!book-blck] ## Red neuronal
>Una Red de Neuronas Artificial podrÃ­a definirse como un grafo cuyos nodos estÃ¡n constituidos por unidades de proceso idÃ©nticas, y que propagan informaciÃ³n a travÃ©s de los arcos.
>
>En este grafo se distinguen tres tipos de nodos: los de entrada, los de salida y los intermedios.

## FÃ³rmula
$$
E = X^TW
$$

>La salida es igual a la transpuesta de la matriz de entradas multiplicando la matriz del peso

Las seÃ±ales E son procesadas ademÃ¡s por una funciÃ³n llamada funciÃ³n de activaciÃ³n o de salida ğ“•, que produce la seÃ±al de salida de la neurona S. Dependiendo de la funciÃ³n ğ“•, habrÃ¡, distintos modelos de autÃ³matas; por ejemplo:

- Lineal: S = KE con K constante.
- Umbral: S = 1 si E >= ğœƒ, S = 0 si E < ğœƒ; siendo ğœƒ el umbral constante.
- Cualquier funciÃ³n: S = ğ“•(I), siendo ğ“• una funciÃ³n cualquiera.

>ğ“•  = FunciÃ³n de activaciÃ³n

-----
## CÃ©lula McCulloch-Pitts

Diapositiva 151

## DefiniciÃ³n - Red neuronal

>[!book-blck] Red Neuronal
>Una red neuronal es una colecciÃ³n de neuronas de McCulloch y Pitts, todas con las mismas escalas de tiempos, donde sus salidas estÃ¡n conectadas a las entradas de otras neuronas.

Una red neuronal de cÃ©lulas de McCulloch-Pitts tiene la capacidad de computaciÃ³n universal. 
Es decir, cualquier estructura que pueda ser programada en una computadora, puede ser modelada mediante una red de cÃ©lulas de McCulloch-Pitts.

>[!example] Ejercicio
>Comprobar AND


| Entrada (X) | Peso(w) | âˆ‘Xw                      | Umbral | Salida (S) |
| ----------- | ------- | ------------------------ | ------ | ---------- |
| 0,0         | 1       | (0)(1) = 0<br>(0)(1) = 0 | 1      | 0          |
| 0,1         | 1       | (0)(1) = 0<br>(1)(1) = 1 | 1      | 0          |
| 1,0         | 1       | ()                       | 1      | 0          |
| 1,1         | 1       | (1)(1) = 1<br>(1)(1) = 1 | 1      | 1          |

------
## PerceptrÃ³n

El modelo se concibiÃ³ como un sistema capaz de realizar tareas de clasificaciÃ³n de forma automÃ¡tica. 
La idea es disponer de un sistema que, a partir de un conjunto de ejemplos de clases diferentes (patrones), fuera capaz de determinar las ecuaciones de las superficies que hacÃ­an de frontera de dichas clases.Â 

La arquitectura de la **==red==** es  de una estructura ==**monocapa**==, en la que hay **un conjunto de cÃ©lulas de entrada**, tantas como sea necesario, segÃºn los tÃ©rminos del problema; y **una o varias cÃ©lulas de salida**. Cada una de las cÃ©lulas de entrada tiene conexiones con todas las cÃ©lulas de salida, y son estas conexiones las que determinan las superficies de discriminaciÃ³n del sistema.

$$
ğ‘¦^â€²=âˆ‘(ğ‘–=1)^ğ‘›  ğ‘¥_ğ‘– ğ‘¤_ğ‘– 
$$

$$
y=F(y^â€²,Î˜)
$$


----
## Entrenamiento supervisado

>Ejemplo con compuerta AND

Y = valor esperado

| X1     | X2     | Y      |
| ------ | ------ | ------ |
| ==-1== | ==-1== | ==-1== |
| ==-1== | ==1==  | ==-1== |
| 1      | -1     | -1     |
| 1      | 1      | 1      |
W1  = 6
W2 = 8
Î˜ = 2

âˆ†W1 = 1
âˆ†W2 = -1
âˆ†Î˜ = -1

W1 = 7
W2 = 7
Î˜ = 1

| Renglon | Formula    | Salida | ğ“• = 1 si y>0<br>ğ“• = -1 caso contrario | Esperado<br>ğ‘‘(ğœ’) |
| ------- | ---------- | ------ | --------------------------------------- | ------------------ |
| 1       | -7 + -7 +1 | -13    | -1                                      | -1                 |
| 2       | -7 + 7+ 1  | 1      | 1                                       | -1                 |
| 3       |            |        |                                         | -1                 |
| 4       |            |        |                                         | 1                  |

âˆ†W1 = 1
âˆ†W2 = -1
âˆ†Î˜ = -1

W1 = 8
W2 = -6
Î˜ = 0

| X1  | X2  | Y   |
| --- | --- | --- |
| -1  | -1  | -1  |
| -1  | 1   | -1  |
| 1   | -1  | -1  |
| 1   | 1   | 1   |

| Renglon | Formula (X1 x W1 + X2 x W2 + Î˜) | Salida | ğ“• = 1 si y>0<br>ğ“• = -1 caso contrario | Esperado<br>ğ‘‘(ğœ’) |
| ------- | ------------------------------- | ------ | --------------------------------------- | ------------------ |
| 1       | -8+6                            | -2     | -1                                      | -1                 |
| 2       |                                 |        |                                         | -1                 |
| 3       |                                 |        |                                         | -1                 |
| 4       |                                 |        |                                         | 1                  |

### Pasos a seguir

1. Empezar con valores aleatorios para los pesos y el umbral.
2. Seleccionar un vector de entrada ğœ’ del conjunto de ejemplos de entrenamiento.
3. Si ğ‘¦â‰ ğ‘‘(ğœ’), la red da una respuesta incorrecta. Modificar $ğ‘¤_ğ‘–$ de acuerdo con:

$$âˆ†ğ‘¤_ğ‘–=ğ‘‘(ğœ’) ğ‘¥_ğ‘–$$

4. Si no se ha cumplido el criterio de finalizaciÃ³n, volver a 2.

----
# ADALINE

ADALINE: Adaptive Linear Neuron.
Su estructura es casi idÃ©ntica a la del PERCEPTRON.

En este modelo se toman en cuenta el error existente entre la salida obtenida y la salida esperada  $|ğ‘‘^ğ‘âˆ’ğ‘¦^ğ‘ |$ .

A este regla de aprendizaje se le conoce como regla Delta.
Normalmente utilizamos el error cuadrÃ¡tico medio:
(Diapositiva 159)

Dado que necesitamos minimizar el error es decir $âˆ†_ğ‘ ğ‘¤_ğ‘—$ tendremos que hacer uso del calculo diferencial para encontrar ese mÃ­nimo.

$$
âˆ†_p w_j=-Î³ \frac{(âˆ‚E^p)}{(âˆ‚w_j )}
$$

$$
âˆ†_p w_j=Î³(d^p-y^p ) x_j
$$

### Procedimiento de Aprendizaje

1. Iniciar con pesos aleatorios.
2. Introducir un patrÃ³n de entrada.
3. Calcular $y^p$, compararla con $d^p$ y obtener su diferencia.
4. Para todos los w encontrar $âˆ†w$ y ponderarla por la tasa de aprendizaje $ğ›¾$.
5. Modificar los pesos con los deltas obtenidos en el paso 4.
6. Si no se cumple con el criterio de convergencia regresar a 2.


-----
# Machine Learning

Debemos tener datos de
- Entrenamiento
- ValidaciÃ³n

![[MachineLearning-IdeaPrincipal.png]]


----
# Numpy

Arreglos
Matrices
Agregar elementos (append) 
Eje 0 â†’ Renglones
Eje 1 â†’ Columnas

>Considerando `numpy` con alias `np`

Insertar elementos en un arreglo

```python
nombre_arreglo = np.insert(nombre_arreglo, posicion, elemento)
```

Buscar informaciÃ³n dentro un arreglo en Numpy
FunciÃ³n `where`

```Python
mayores0 = np.where(miArreglo2>0)
```

Operaciones vectoriales con arreglos de Numpy
Afectan a todos los elementos del arreglo

```python
miVector+=1
```

DesviaciÃ³n 

```Python
desviacion = miVector.std()
```


----
# ğŸ¼Pandas

Importar pandas y cargar archivo CSV

```Python
import pandas as pd
datos = pd.read_csv("Ruta_CSV")
print datos
```

Para convertir de CSV a arreglo de Numpy

```Python
datosNumpy = np.array(datos.values)
```

## Filtros

Imprimir fechas con temperatura promedio mayor a 20 â„ƒ

```Python
filtro = fechas [tempProm>20]
```

Transpuesta

```Python
totalDatos = totalDatos.transpose()
```

## Data frame

```
pd.Series
```

>Nos quedamos en introducciÃ³n de pandas 1

Se puede recorrer con un for

Para acceder al dato por un "apuntador" se usa la funciÃ³n `iloc[<indice>]`
>Ejemplo
>`nombreArreglo.iloc[0]`

Para cambiar el tipo de datos, se puede usar la funciÃ³n `dtype`

Para ordenar los datos, se usa la funciÃ³n `sort`

Medidas de tendencia central (media mediana y moda)

Media â¡ï¸ `.mean()`
Mediana â¡ï¸ `.median()`
Moda â¡ï¸ `.mode()`

Una matriz, se pueden hacer dos arreglos (uno de filas y uno de columnas)

```Python
indices = ["Renglon1","Renglon2","Renglon3"]
columnas = ["Columna1","Columna2","Columna3"]

datos = [[],[],[],[],[],[],[],[],[]]

```

TambiÃ©n se puede poner con un diccionario 

`.head()` â¡ï¸ nos pone los primeros 5 renglones
`.tail()` â¡ï¸ nos pone los Ãºltimos 5

Para insertar un renglÃ³n â¡ï¸

```python
pd.Series(name="", data=<datos>, index =["Columna1","Columna2","Columna3"] )
```

---
# Problemas de ClasificaciÃ³n
## Descenso del gradiente

>[!swords] Ejemplo IBM
>[Â¿QuÃ© es el descenso del gradiente? | IBM](https://www.ibm.com/mx-es/topics/gradient-descent)

Este mÃ©todo busca los mejores pesos y sesgos que minimizan la pÃ©rdida, y elimina los valores atÃ­picos del conjunto de datos. 
El objetivo del descenso de gradiente es reducir el error o la funciÃ³n de costo de un modelo, al realizar pruebas con una variable de entrada y el resultado esperado.

### DescensoÂ del gradiente por lotes  

El descensoÂ del gradienteÂ por lotes suma el error de cada punto en un conjunto de entrenamiento, actualizando el modelo solo despuÃ©s de queÂ se hayan evaluadoÂ todos los ejemplos de entrenamiento. Este proceso se conoce como Ã©poca de entrenamiento.

Si bien este procesamiento por lotes proporciona eficiencia de cÃ¡lculo, aÃºn puede requerir un tiempo de procesamiento prolongado para grandes conjuntos de datos de entrenamiento, ya que aÃºn necesita almacenar todos los datos en la memoria. El descenso del gradiente por lotes tambiÃ©n suele producir un gradiente de error estable y una convergencia, pero a veces ese punto de convergencia no es el mÃ¡s ideal, ya que encuentra el mÃ­nimo local frente al global.

### Descenso delÂ gradiente estocÃ¡stico  

El descensoÂ  del gradiente estocÃ¡sticoÂ (SGD, sigla en inglÃ©s de stochastic gradient descent) ejecuta una Ã©poca de entrenamiento para cada ejemplo dentro del conjunto de datos y actualizaÂ los parÃ¡metros de cada ejemplo de entrenamiento uno a la vez. Dado que solo es necesario tener un ejemplo de entrenamiento, son mÃ¡s fÃ¡ciles de almacenar en la memoria. Si bien estas actualizaciones frecuentes pueden ofrecer mÃ¡s detalles y velocidad, pueden generar pÃ©rdidas en la eficiencia computacional en comparaciÃ³n con el descenso del Â gradiente por lotes. Sus actualizaciones frecuentes pueden generar gradientes ruidosos, pero esto tambiÃ©n puede ser Ãºtil para escapar del mÃ­nimo local y encontrar el global.

### Descenso delÂ gradiente por mini lotes  

 El descenso delÂ gradiente por mini lotesÂ combina conceptos tanto del descenso del gradiente por lotes como del descenso del gradiente estocÃ¡stico. Divide elÂ conjunto de datos de entrenamientoÂ en lotes pequeÃ±osÂ y realiza actualizaciones en cada uno de esos lotes. Este enfoque logra un equilibrio entre la eficiencia computacional del descenso delÂ gradiente por lotesÂ y la velocidad del descenso delÂ gradiente estocÃ¡stico.

>[!example] Ejemplo MNNIST
>[El dataset MNIST | Interactive Chaos](https://interactivechaos.com/es/manual/tutorial-de-deep-learning/el-dataset-mnist)

Dataset: MNNIST

-----
## MÃ¡quinas de vector soporte (SVM)

>[!cpu-blck] Ejemplo SVM  â†’ IBM
>[Funcionamiento de SVM - DocumentaciÃ³n de IBM](https://www.ibm.com/docs/es/spss-modeler/saas?topic=models-how-svm-works)

Puede hacer
- Clasificaciones lineales
- Clasificaciones no lineales
- Regresiones
- DetecciÃ³n de valores atÃ­picos

>[!caution] Se tienen que escalar y normalizar los datos

### Tipos
#### Clasificadores de margen duro
Cuando forzamos al modelo para que todos los datos de entrenamiento quedan fuera del vector soporte
>Ejemplo una calle muy angosta

#### Clasificadores de margen blando
Cuando el margen entre los vectores de soporte es buen aunque algunos datos de entrenamiento caigan en el
>Ejemplo, una avenida mÃ¡s ancha 

### Hiperparametros de un SVM

C: distancia entre los vectores de soporte
- Un valor bajo de C nos da una distancia grande
- Un valor alto de C nos darÃ¡ una baja distancia

----
## Pasos para crear un Modelo SVM

1. Obtener datos y clase de entrenamiento
2. Escalar los datos de entrenamiento 
	1. Normalmente se usa la funciÃ³n ==StandardScaler==
3. Crear el modelo linear SVC
4. Entrenar el modelo con los datos y las clases de entrenamiento
5. Realizar predicciones con el modelo entrenado.

### StandardScaler
Normaliza todos los datos
- Establece la media en 0
- Establece la desviaciÃ³n estÃ¡ndar en 1

>[!book-blck] Ejemplo
>[LinearSVC â€” scikit-learn 1.5.2 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)

Nos quedamos en la Diapositiva del SVM del iris (graficaciÃ³n del SVM )

----
## SVM como clasificadores no lineales

Para hacer esto podemos usar un transformador `PolynomialFeatures`; posteriormente se realiza 

Ejemplo:

```Python
polynomial=Pipelenie([("poly_features",PolynomialFeatures(degree=3))
("scaler",StandardScaler()),
("svm_clf",LinearSVC(C-10,loss="hinge"))
])
```

Otra forma de manejar caracterÃ­sticas polinomiales es modificar el kernel del modelo de lineal a polinomial

```Python
poly_kernel_svm=Pipeline([
("scaler",StandardScaler()),
("svm_clf",SVC(kernel="poly",degree=3,coef0=1,C=5))
])
poly_kernel_svm.fit(x,y)
```

Kernel de funciÃ³n de base radial gaussiana

```Python
rbf_kernel_svm_clf=Pipeline([
("scaler",StandardScaler()),

])
```

----
## Regresiones con SVM

Este modelo soporta regresiones lineales y no lineales
En los modelos de regresiÃ³n la idea es que la mayorÃ­a de los datos de entrenamiento queden localizados entre los dos vectores de soporte.
La distancia entres los vectores de soporte esta dada por un hiperparametro llamado É› (epsilon)

----

>DataSet de Obesidad
>[Obesity Dataset](https://www.kaggle.com/datasets/suleymansulak/obesity-dataset)


-----
# Ãrboles de decisiÃ³n

Son un modelo de ML en donde podemos:
- ClasificaciÃ³n
- RegresiÃ³n

Son el componente bÃ¡sico de los bosques aleatorios
No requieren escalado ni centrado de datos

Scikit-Learn utiliza el algoritmo `CART` para generar los Ã¡rboles, por lo que estos solo pueden ser Ã¡rboles binarios.

## Algoritmos
### Impureza de GINI
Un nodo es puro si gini = 0; es decir, si todas las instancias de entrenamiento de ese nodo pertenecen a la misma clase

>Por default es el algoritmo que se utiliza
### EntropÃ­a
La entropÃ­a de un nodo es 0 cuando se contienen instancias de una sola clase.

>[!example] Ejemplo
>Se usa el dataset iris

```Python
from sklearn.datasets import load_iris
from sklearn.tree import DecissionTreeClassifier

iris = load_iris()
```

Para calcular las probabilidades de que una instancia pertenezca a una clase en concreto se usa el siguiente comando:

```Python
tree_clf.predict_proba([[5,1.5]])
```

---
## Algoritmo CART (Classification and Regression Tree)

- Divide el conjunto de entrenamiento en dos subconjuntos mediante una sola caracterÃ­stica K y un umbral $t_k$ 
- Selecciona los parÃ¡metros $k$ y $t_k$ que produzcan los conjuntos mÃ¡s puros
- El algoritmo trabaja de manera ==recursiva== hasta completar la profundidad con la que fue configurado el modelo.
- Este algoritmo da una soluciÃ³n razonablemente buena pero ==**no asegura que sea Ã³ptima**== 
- La velocidad para realizar predicciones es buena ya que solo se tiene que recorre el Ã¡rbol, el cual generalmente estÃ¡ balanceado.

Los Ã¡rboles de decision, por naturaleza tienden a sobre-ajustar los datos (*overfitting*)

### Hiper-parÃ¡metros
- Max depth: profundidad del Ã¡rbol
- Min samples split nÃºmero de mÃ­nimo de muestras que debe de tener el Ã¡rbol
- Min samples leaf: nÃºmero mÃ¡ximo de muestras de un nodo terminal
- Max leaf nodes: numero mÃ¡ximo de nodos terminales
- Max features: nÃºmero mÃ¡ximo de caracterÃ­sticas a evaluar para dividir un nodo

----
## RegresiÃ³n con Ã¡rboles de decisiÃ³n
Se usa la clase `DecissionTreeRegressor`

- El valor de la predicciÃ³n es el valor medio de todas las instancias de ese nodo
- El nodo terminal tambiÃ©n nos arroja el erro cuadrÃ¡tico medio.
- En el caso de la regresiÃ³n, el algoritmo CART no intenta minimizar la impureza; sino lo que intenta minimizar es el ECM (error cuadrÃ¡tico medio)

>[!caution] NO es recomendable dejarlo sin restricciones, se recomienda poner un nÃºmero mÃ¡ximo de hojas

